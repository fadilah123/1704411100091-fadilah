



<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      <meta http-equiv="x-ua-compatible" content="ie=edge">
      
        <meta name="description" content="Penjelasan Tentang K-Means Clustering And K-Nearest neighbor">
      
      
        <link rel="canonical" href="https://sitimusabbiahtulfadilah1234.github.io/170441100091-fadilah/K-NN/">
      
      
        <meta name="author" content="siti musabbihatul fadilah">
      
      
        <meta name="lang:clipboard.copy" content="Copy to clipboard">
      
        <meta name="lang:clipboard.copied" content="Copied to clipboard">
      
        <meta name="lang:search.language" content="en">
      
        <meta name="lang:search.pipeline.stopwords" content="True">
      
        <meta name="lang:search.pipeline.trimmer" content="True">
      
        <meta name="lang:search.result.none" content="No matching documents">
      
        <meta name="lang:search.result.one" content="1 matching document">
      
        <meta name="lang:search.result.other" content="# matching documents">
      
        <meta name="lang:search.tokenizer" content="[\s\-]+">
      
      <link rel="shortcut icon" href="../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.0.4, mkdocs-material-4.2.0">
    
    
      
        <title>KNN data iris - K-Means Clustering And K-Nearest neighbor</title>
      
    
    
      <link rel="stylesheet" href="../assets/stylesheets/application.750b69bd.css">
      
        <link rel="stylesheet" href="../assets/stylesheets/application-palette.224b79ff.css">
      
      
        
        
        <meta name="theme-color" content="#3f51b5">
      
    
    
      <script src="../assets/javascripts/modernizr.74668098.js"></script>
    
    
      
        <link href="https://fonts.gstatic.com" rel="preconnect" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,400,400i,700|Roboto+Mono">
        <style>body,input{font-family:"Roboto","Helvetica Neue",Helvetica,Arial,sans-serif}code,kbd,pre{font-family:"Roboto Mono","Courier New",Courier,monospace}</style>
      
    
    <link rel="stylesheet" href="../assets/fonts/material-icons.css">
    
    
    
      
        
<script>
  window.ga = window.ga || function() {
    (ga.q = ga.q || []).push(arguments)
  }
  ga.l = +new Date
  /* Setup integration and send page view */
  ga("create", "None", "auto")
  ga("set", "anonymizeIp", true)
  ga("send", "pageview")
  /* Register handler to log search on blur */
  document.addEventListener("DOMContentLoaded", () => {
    if (document.forms.search) {
      var query = document.forms.search.query
      query.addEventListener("blur", function() {
        if (this.value) {
          var path = document.location.pathname;
          ga("send", "pageview", path + "?q=" + this.value)
        }
      })
    }
  })
</script>
<script async src="https://www.google-analytics.com/analytics.js"></script>
      
    
    
  </head>
  
    
    
    <body dir="ltr" data-md-color-primary="indigo" data-md-color-accent="indigo">
  
    <svg class="md-svg">
      <defs>
        
        
          <svg xmlns="http://www.w3.org/2000/svg" width="416" height="448"
    viewBox="0 0 416 448" id="__github">
  <path fill="currentColor" d="M160 304q0 10-3.125 20.5t-10.75 19-18.125
        8.5-18.125-8.5-10.75-19-3.125-20.5 3.125-20.5 10.75-19 18.125-8.5
        18.125 8.5 10.75 19 3.125 20.5zM320 304q0 10-3.125 20.5t-10.75
        19-18.125 8.5-18.125-8.5-10.75-19-3.125-20.5 3.125-20.5 10.75-19
        18.125-8.5 18.125 8.5 10.75 19 3.125 20.5zM360
        304q0-30-17.25-51t-46.75-21q-10.25 0-48.75 5.25-17.75 2.75-39.25
        2.75t-39.25-2.75q-38-5.25-48.75-5.25-29.5 0-46.75 21t-17.25 51q0 22 8
        38.375t20.25 25.75 30.5 15 35 7.375 37.25 1.75h42q20.5 0
        37.25-1.75t35-7.375 30.5-15 20.25-25.75 8-38.375zM416 260q0 51.75-15.25
        82.75-9.5 19.25-26.375 33.25t-35.25 21.5-42.5 11.875-42.875 5.5-41.75
        1.125q-19.5 0-35.5-0.75t-36.875-3.125-38.125-7.5-34.25-12.875-30.25-20.25-21.5-28.75q-15.5-30.75-15.5-82.75
        0-59.25 34-99-6.75-20.5-6.75-42.5 0-29 12.75-54.5 27 0 47.5 9.875t47.25
        30.875q36.75-8.75 77.25-8.75 37 0 70 8 26.25-20.5
        46.75-30.25t47.25-9.75q12.75 25.5 12.75 54.5 0 21.75-6.75 42 34 40 34
        99.5z" />
</svg>
        
      </defs>
    </svg>
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" data-md-component="overlay" for="__drawer"></label>
    
    
      <header class="md-header" data-md-component="header">
  <nav class="md-header-nav md-grid">
    <div class="md-flex">
      <div class="md-flex__cell md-flex__cell--shrink">
        <a href="https://sitimusabbiahtulfadilah1234.github.io/170441100091-fadilah/" title="K-Means Clustering And K-Nearest neighbor" class="md-header-nav__button md-logo">
          
            <i class="md-icon"></i>
          
        </a>
      </div>
      <div class="md-flex__cell md-flex__cell--shrink">
        <label class="md-icon md-icon--menu md-header-nav__button" for="__drawer"></label>
      </div>
      <div class="md-flex__cell md-flex__cell--stretch">
        <div class="md-flex__ellipsis md-header-nav__title" data-md-component="title">
          
            <span class="md-header-nav__topic">
              K-Means Clustering And K-Nearest neighbor
            </span>
            <span class="md-header-nav__topic">
              KNN data iris
            </span>
          
        </div>
      </div>
      <div class="md-flex__cell md-flex__cell--shrink">
        
          <label class="md-icon md-icon--search md-header-nav__button" for="__search"></label>
          
<div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="query" data-md-state="active">
      <label class="md-icon md-search__icon" for="__search"></label>
      <button type="reset" class="md-icon md-search__icon" data-md-component="reset" tabindex="-1">
        &#xE5CD;
      </button>
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="result">
          <div class="md-search-result__meta">
            Type to start searching
          </div>
          <ol class="md-search-result__list"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
        
      </div>
      
        <div class="md-flex__cell md-flex__cell--shrink">
          <div class="md-header-nav__source">
            


  

<a href="https://github.com/sitimusabbiahtulfadilah1234/170441100091-fadilah" title="Go to repository" class="md-source" data-md-source="github">
  
    <div class="md-source__icon">
      <svg viewBox="0 0 24 24" width="24" height="24">
        <use xlink:href="#__github" width="24" height="24"></use>
      </svg>
    </div>
  
  <div class="md-source__repository">
    sitimusabbiahtulfadilah1234/170441100091-fadilah
  </div>
</a>
          </div>
        </div>
      
    </div>
  </nav>
</header>
    
    <div class="md-container">
      
        
      
      
        

<nav class="md-tabs" data-md-component="tabs">
  <div class="md-tabs__inner md-grid">
    <ul class="md-tabs__list">
      
        
  <li class="md-tabs__item">
    
      <a href=".." title="K-Means  data iris" class="md-tabs__link md-tabs__link--active">
        K-Means  data iris
      </a>
    
  </li>

      
        
      
        
      
    </ul>
  </div>
</nav>
      
      <main class="md-main">
        <div class="md-main__inner md-grid" data-md-component="container">
          
            
              <div class="md-sidebar md-sidebar--primary" data-md-component="navigation">
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    <nav class="md-nav md-nav--primary" data-md-level="0">
  <label class="md-nav__title md-nav__title--site" for="__drawer">
    <a href="https://sitimusabbiahtulfadilah1234.github.io/170441100091-fadilah/" title="K-Means Clustering And K-Nearest neighbor" class="md-nav__button md-logo">
      
        <i class="md-icon"></i>
      
    </a>
    K-Means Clustering And K-Nearest neighbor
  </label>
  
    <div class="md-nav__source">
      


  

<a href="https://github.com/sitimusabbiahtulfadilah1234/170441100091-fadilah" title="Go to repository" class="md-source" data-md-source="github">
  
    <div class="md-source__icon">
      <svg viewBox="0 0 24 24" width="24" height="24">
        <use xlink:href="#__github" width="24" height="24"></use>
      </svg>
    </div>
  
  <div class="md-source__repository">
    sitimusabbiahtulfadilah1234/170441100091-fadilah
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
      


  <li class="md-nav__item">
    <a href=".." title="K-Means data iris" class="md-nav__link">
      K-Means  data iris
    </a>
  </li>

    
      
      
      

  


  <li class="md-nav__item md-nav__item--active">
    
    <input class="md-toggle md-nav__toggle" data-md-toggle="toc" type="checkbox" id="__toc">
    
    
    <a href="./" title="KNN data iris" class="md-nav__link md-nav__link--active">
      KNN data iris
    </a>
    
  </li>

    
      
      
      


  <li class="md-nav__item">
    <a href="../pohon_keputusan/" title="Pohon Keputusan" class="md-nav__link">
      Pohon Keputusan
    </a>
  </li>

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
          
          <div class="md-content">
            <article class="md-content__inner md-typeset">
              
                
                
                  <h1>KNN data iris</h1>
                
                <p>Pengertian, Pengenalan, dan Contoh Implementasi K-Nearest Neighbor (KNN) </p>
<p>\1. Pendahuluan</p>
<p>Pada sekarang ini kebutuhan akan informasi semakin meningkat seiring dengan berkembangnya teknologi dalam menyebarkan informasi kepada masyarakat. Informasi yang dibutuhkan mengalami banyak perkembangan mulai dari informasi yang bersifat umum hingga informasi yang bersifat khusus. Banyaknya informasi dan dokumen yang tersedia mendorong pengguna untuk mencari cara lebih cepat dalam mendapatkan informasi dan dokumen yang dibutuhkan. Jika waktu pencarian terlalu lama, maka manfaat dari informasi yang diperoleh dapat berkurang. Hal ini dikarenakan informasi yang diperoleh sudah masuk waktu yang sudah tidak berguna atau tidak valid.</p>
<p>Klasifikasi dokumen dapat membantu proses pencarian sebuah dokumen dengan cepat dan tepat. Klasifikasi dokumen mengelompokan dokumen yang sesuai dengan katergori yang terkandung pada dokumen tersebut. Permasalahan klasifikasi dokumen bisa diselesaikan dengan banyak metode, salah satu diantaranya adalah K-Nearest Neighbor (KNN). Algoritma KNN merupakan sebuah metode untuk melakukan klasifikasi terhadap objek yang berdasarkan dari data yang jaraknya paling dekat dengan objek tersebut. Algoritma KNN merupakan sebuah metode untuk melakukan klasifikasi terhadap objek berdasarkan data pembelajaran yang jaraknya paling dekat dengan objek tersebut.Pengertian dan Cara Kerja Algoritma K-Nearest Neighbors (KNN)</p>
<p><img alt="" src="D:\semester 4\data mining\tgs3\mkdocs-material-master\docs\assets\images\A.png" /></p>
<p>K-nearest neighbors atau knn adalah algoritma yang berfungsi untuk melakukan klasifikasi suatu data berdasarkan data pembelajaran (<em>train data sets</em>), yang diambil dari k tetangga terdekatnya (<em>nearest neighbors</em>). Dengan k merupakan banyaknya tetangga terdekat.</p>
<p><strong>A. Cara Kerja Algoritma K-Nearest Neighbors (KNN)</strong><a href="https://najibtombro.github.io/170441100101_FIRDHO-ARIANZA_KNN-/#a-cara-kerja-algoritma-k-nearest-neighbors-knn"><strong>¶</strong></a></p>
<p>K-nearest neighbors melakukan klasifikasi dengan proyeksi data pembelajaran pada ruang berdimensi banyak. Ruang ini dibagi menjadi bagian-bagian yang merepresentasikan kriteria data pembelajaran. Setiap data pembelajaran direpresentasikan menjadi titik-titik <strong>c</strong> pada ruang dimensi banyak.</p>
<p><strong>KLASIFIKASI TERDEKAT (NEAREST NEIGHBOR CLASSIFICATION)</strong><a href="https://najibtombro.github.io/170441100101_FIRDHO-ARIANZA_KNN-/#klasifikasi-terdekat-nearest-neighbor-classification"><strong>¶</strong></a></p>
<p><strong>Data baru</strong> yang diklasifikasi selanjutnya diproyeksikan pada ruang dimensi banyak yang telah memuat titik-titik c data pembelajaran. Proses klasifikasi dilakukan dengan mencari titik <strong>c**terdekat dari **c-baru</strong> (<em>nearest neighbor</em>)<em>.</em> Teknik pencarian tetangga terdekat yang umum dilakukan dengan menggunakan formula jarak euclidean*.* Berikut beberapa formula yang digunakan dalam algoritma knn.</p>
<p>·         <strong>Euclidean Distance</strong><a href="https://najibtombro.github.io/170441100101_FIRDHO-ARIANZA_KNN-/#euclidean-distance"><strong>¶</strong></a></p>
<p>Jarak Euclidean adalah formula untuk mencari jarak antara 2 titik dalam ruang dua dimensi.</p>
<p>·         <strong>Hamming Distance</strong><a href="https://najibtombro.github.io/170441100101_FIRDHO-ARIANZA_KNN-/#hamming-distance"><strong>¶</strong></a></p>
<p>Jarak Hamming adalah cara mencari jarak antar 2 titik yang dihitung dengan panjang vektor biner yang dibentuk oleh dua titik tersebut dalam block kode biner.</p>
<p>·         <strong>Manhattan Distance</strong><a href="https://najibtombro.github.io/170441100101_FIRDHO-ARIANZA_KNN-/#manhattan-distance"><strong>¶</strong></a></p>
<p>Manhattan Distance atau Taxicab Geometri adalah formula untuk mencari jarak <strong>d</strong> antar 2 vektor <strong>p,q</strong> pada ruang dimensi <strong>n*</strong>.*</p>
<p>·         <strong>Minkowski Distance</strong><a href="https://najibtombro.github.io/170441100101_FIRDHO-ARIANZA_KNN-/#minkowski-distance"><strong>¶</strong></a></p>
<p>Minkowski distance adalah formula pengukuran antar 2 titik pada ruang vektor normal yang merupakan hibridisasi yang mengeneralisasi euclidean distance dan mahattan distance.</p>
<p>Teknik pencarian tetangga terdekat disesuaikan dengan dimensi data, proyeksi, dan kemudahan implementasi oleh pengguna.</p>
<p><strong>BANYAKNYA K TETANGGA TERDEKAT</strong><a href="https://najibtombro.github.io/170441100101_FIRDHO-ARIANZA_KNN-/#banyaknya-k-tetangga-terdekat"><strong>¶</strong></a></p>
<p>Untuk menggunakan algoritma k nearest neighbors, perlu ditentukan banyaknya k tetangga terdekat yang digunakan untuk melakukan klasifikasi data baru. Banyaknya k, sebaiknya merupakan angka ganjil, misalnya k = 1, 2, 3, dan seterusnya. Penentuan nilai k dipertimbangkan berdasarkan banyaknya data yang ada dan ukuran dimensi yang dibentuk oleh data. Semakin banyak data yang ada, angka k yang dipilih sebaiknya semakin rendah. Namun, semakin besar ukuran dimensi data, angka k yang dipilih sebaiknya semakin tinggi.</p>
<p><strong>ALGORITMA K-NEAREST NEIGHBORS</strong><a href="https://najibtombro.github.io/170441100101_FIRDHO-ARIANZA_KNN-/#algoritma-k-nearest-neighbors"><strong>¶</strong></a></p>
<p>\1.    Tentukan k bilangan bulat positif berdasarkan ketersediaan data pembelajaran.</p>
<p>\2.    Pilih tetangga terdekat dari data baru sebanyak k.</p>
<p>\3.    Tentukan klasifikasi paling umum pada langkah (ii), dengan menggunakan frekuensi terbanyak.</p>
<p>\4.    Keluaran klasifikasi dari data sampel baru.</p>
<p>​    </p>
<p><strong>B. Contoh Aplikasi K Nearest Neighbor</strong><a href="https://najibtombro.github.io/170441100101_FIRDHO-ARIANZA_KNN-/#b-contoh-aplikasi-k-nearest-neighbor"><strong>¶</strong></a></p>
<p>Contoh berikut diambil dari buku “<strong>Data Science Algorithms in a Week</strong>” yang ditulis oleh Dávid Natingga.</p>
<p>Pada contoh ini, dilakukan klasifikasi suhu udara berdasarkan persepsi seseorang yang bernama Marry. Adapun klasifikasi suhu udara terdiri dari 2 persepsi yaitu <strong>Panas</strong> dan <strong>Dingin</strong>. Persepsi ini dapat diukur berdasarkan 2 variabel yaitu <strong>temperatur dalam derajat celcius</strong> dan <strong>kecepatan</strong> <strong>angin dalam km/h</strong>. Diperoleh data berikut,</p>
<p>Untuk contoh ini terbentuk ruang dimensi 2, yang berisi 2 kriteria yaitu <strong>temperatur udara</strong> dan <strong>kecepatan angin</strong>.</p>
<p><img alt="" src="../assets/images/B.png" /></p>
<p>Pada proyeksi di atas sumbu vertikal adalah kecepatan angin, sumbu horizontal adalah temperatur suhu, warna biru adalah dingin, dan warna merah adalah panas.</p>
<p>Dari proyeksi diatas, dapat dilakukan klasifikasi data baru. Misalnya, Bagaimana persepsi Marry saat temperatur udara 16°C dan kecepatan angin 3km/jam.</p>
<p><img alt="" src="D:\semester 4\data mining\tgs3\mkdocs-material-master\docs\assets\images\D.png" /></p>
<p>Proses pencarian tetangga terdekat</p>
<p><img alt="" src="D:\semester 4\data mining\tgs3\mkdocs-material-master\docs\assets\images\E.png" /></p>
<p>Dapat diketahui tetangga terdekatnya adalah titik <strong>c dingin</strong> dengan temperature 15°C dan kecepatan angin 5km/jam. Jadi berdasarkan pemilihan k = 1, klasifikasinya adalah <strong>dingin</strong>.</p>
<p>Dengan melakukan proses di atas terhadap semua titik, diperoleh proyeksi klasifikasi berikut.</p>
<p><img alt="" src="D:\semester 4\data mining\tgs3\mkdocs-material-master\docs\assets\images\F.png" /></p>
<p><strong>Catatan: Untuk pemilihan k lainnya, hasil klasifikasi ditentukan dengan frekuensi terbanyak. Misalnya k = 3, dengan titik terdekat dingin, panas, dingin. Hasil klasifikasi data baru tersebut adalah dingin.</strong></p>
<p>\2. Pengertian K-Nearest Neighbor (KNN)<a href="https://najibtombro.github.io/170441100101_FIRDHO-ARIANZA_KNN-/#2-pengertian-k-nearest-neighbor-knn">¶</a></p>
<p>KNN adalah salah satu metode dimana metode ini melakukan klasifikasi berdasarkan data training atau data pembelajaran dilihat dari jarak yang paling dekat dengan objek berdasarkan nilai k. Metode ini bertujuan untuk mengklasifikasikan objek baru berdasarkan atribut dan training sample. Diberikan suatu titik query, selanjutnya akan ditemukan sejumlah K objek atau titik training yang paling dekat dengan titik query. Nilai prediksi dari query akan ditentukan berdasarkan klasifikasi tetanggaa Algoritma K-Nearest Neighbor (K-NN)** adalah sebuah metode klasifikasi terhadap sekumpulan data berdasarkan pembelajaran data yang sudah terklasifikasikan sebelumya. Termasuk dalam <strong>supervised learning</strong>, dimana hasil query instance yang baru diklasifikasikan berdasarkan mayoritas kedekatan jarak dari kategori yang ada dalam <strong>K-NN</strong>.</p>
<p>Dengan demikian, kelas <em>KNeighborsClassifier</em> dapat digunakan tanpa secara eksplisit menentukan nilai parameter apa pun. Nilai default sudah disediakan. Ini membuatnya mudah untuk menggunakannya untuk tujuan pembelajaran awal dan kemudian menambahkan nilai parameter, satu per satu. Inilah tepatnya yang akan saya lakukan. Kami akan menggunakan dataset iris untuk program demo kami. Kumpulan data Iris tersedia dalam sklearn itu sendiri.</p>
<p>Secara Sederhana K-nearest neighbors atau knn adalah algoritma yang berfungsi untuk melakukan klasifikasi suatu data berdasarkan data pembelajaran (<em>train data sets</em>), yang diambil dari k tetangga terdekatnya (<em>nearest neighbors</em>). Dengan k merupakan banyaknya tetangga terdekat. KNN ini dapat kita temukan bilamana kita sedang belajar Machine Learning. ML intinya berkaitan dengan automasi atau bagaimana sebuah machine dapat belajar dari contoh-contoh yang kita berikan, terus memprediksi sesuatu yang sesuai dengan contoh-contoh tadi.</p>
<p>Lihat gambar di bawah, misalnya terdapat dua kategori yang ditunjukkan dengan Lingkaran Merah (LM) dan Kotak Hijau (KH). Kemudian kita hendak mengetahui kategori dari Bintang Biru (BB). BB dapat berupa LM atau tidak keduanya (gambar sebelah kiri). K adalah jumlah tetangga terdekat yang akan kita gunakan, asumsikan saja k = 3 (gambar sebelah kanan). Tentu dari sini kita bisa dapat segera tahu dengan membuat lingkaran yang memungkinkan BB menjadi di tengah-tengah 3 titik terdekat. Tiga titik terdekat ke BB adalah LM. Oleh karena itu, dengan sangat yakin kita dapat mengatakan bahwa BB mempunyai kelas LM. Dari sini tentu kita sadar, bahwa penentuan nilai K sangat penting pada algoritma KNN.</p>
<p><img alt="" src="../assets/images/G.png" /></p>
<p>ilustrasi KNN</p>
<p>Penentuan jarak terdekat diantara dua titik di atas dapat menggunakan dalil Pythagoras, akan tetapi bilamana terdapat lebih dari dua <em>feature/independent variable</em> kita dapat menggunakan <em>euclidean distance, jaccard, cosine, manhattan, minkowski, dll</em>.</p>
<p><img alt="" src="../assets/images/H.gif" /></p>
<p>Rumus Pythagoras</p>
<p><img alt="" src="../assets/images/I.jpg" /></p>
<p>Beberapa metode pengukuran jarak</p>
<p>Kalau diterapin di SIG (Sistem Informasi Geografis) tentu bisa dong? yup tentu sangat bisa. Misal untuk menentukan lokasi dari dua titik. Lihat gambar di bawah ini untuk lebih jelasnya..</p>
<p><img alt="img" src="file:///C:/Users/TOSHIBA/AppData/Local/Temp/msohtmlclip1/01/clip_image011.png" /></p>
<p>contoh di ranah SIG sederhana</p>
<p>Secara garis besar dalam dunia data mining atau data science terdapat 2 pendekatan untuk melakukan teknik — teknik data mining. <em>Supervised learning</em> adalah sebuah pendekatan dimana sudah terdapat data yang dilatih, dan terdapat variable yang ditargetkan sehingga tujuan dari pendekatan ini adalah mengkelompokan suatu data ke data yang sudah ada, lain halnya dengan unsupervised learning, <em>unsupervised learning</em> tidak memiliki data latih, sehingga dari data yang ada, kita mengelompokan data tersebut menjadi 2 bagian atau 3 bagian dan seterusnya.</p>
<p>Sederhananya, <em>Supervised</em> itu artinya sudah ter*manage* dengan baik (data yang fitur dan labelnya udah jelas). Misal jikalau terdapat ciri-ciri daun yang <em>phyllotaxis</em>-nya berhadapan berseling, <em>circumscriptio</em> berbentuk jorong, <em>apex falii</em> berbentuk runcing, dan <em>nervatio</em> menyirip itu berarti sudah jelas mengkudu (<em>Morinda citrifolia</em>). Sedangkan <em>Unsupervised learning</em> targetnya atau labelnya belum jelas. Metode yang dipakai biasanya <em>Clustering</em>. Jadi kita cuma ngelompokin data yang punya keterkaitan satu sama lain, tanpa tahu mereka sebenernya bener-bener satu label atau enggak. Algoritma <em>Supervised Learning</em> misalnya Decision tree, Nearest — Neighbor Classifier, Naive Bayes Classifier, Artificial Neural Network, dll. Algoritma *Unsupervised Learning*misalnya K-Means, Hierarchical Clustering, DBSCAN, dll.</p>
<p>\3. Iris Datasets<a href="https://najibtombro.github.io/170441100101_FIRDHO-ARIANZA_KNN-/#3-iris-datasets">¶</a></p>
<p>Kali ini saya menggunakan Python untuk melakukan coba-coba model. Disini saya menggunakan dataset iris. Dataset ini sangat populer digunakan untuk latihan pertama (R atau python). Iris biasanya sudah tersedia didalam modul sklearn (lengkap dengan target dan feature) atau jika kita belum install sklearn</p>
<p># code untuk install modul sklearn</p>
<p>pip install -U scikit-learn #jika menggunakan pip yang ada di python</p>
<p>conda install scikit-learn #jika menggunakan anaconda</p>
<p>Terdapat 150 observasi (row) dengan <em>feature/independent variable</em> sebanyak 4 (Panjang sepal, lebar sepal, panjang petal, dan lebar petal). 150 observasi tersebut dibagi menjadi 50 observasi pada masing-masing spesies (<em>Iris setosa</em>, <em>Iris versicolor</em>, dan <em>Iris virginica</em>). Pada data iris, kita tidak akan menjumpai nilai null (N/A), sehingga kita tidak perlu capek-capek untuk merapikan data tersebut.</p>
<p><img alt="" src="../assets/images/K.png" /></p>
<p>Pengukuran petal dan sepal</p>
<p>Pada gambar di bawah saya menggunakan atom dengan data iris yang sudah secara default berada pada modul sklearn. Kita tinggal memanggil data tersebut. Kemudian seperti biasa, kita definisikan variabel untuk memanggil data tersebut, dalam hal ini <strong>iris = load_iris()</strong>.</p>
<p><img alt="img" src="file:///C:/Users/TOSHIBA/AppData/Local/Temp/msohtmlclip1/01/clip_image013.gif" /></p>
<p>scatter plot menggunakan data iris yang sudah tersedia di modul sklearn</p>
<p><img alt="" src="../assets/images/M.png" /></p>
<p>Hasil grafik dari kode menggunakan atom</p>
<p>Untuk gambar di bawah ini saya menggunakan jupyter notebook, dengan data iris hasil download dengan format file csv. Modul Pandas dapat mempermudah kita dalam pemanggilan dataset yang kita butuhkan, untuk lebih jelasnya dapat dibaca <a href="https://pandas.pydata.org/">disini</a>. Penilaian subjektif saya, jika membandingkan atom dan jupyter notebook, saya lebih suka menggunakan jupyter notebook, karena selain ringan dia juga dapat mengeksport code-code yang telah kita tulis menggunakan ekstensi Gist-it ke github yang kita miliki.</p>
<p>In [2]:</p>
<p>import pandas as pd # import file menggunakan pandas</p>
<p>dataset = pd.read_csv('iris.csv')</p>
<p>In [3]:</p>
<p>dataset.head(5) #menampilkan dataset 5 teratas</p>
<p>Out[3]:</p>
<table>
<thead>
<tr>
<th></th>
<th>sepal_length</th>
<th>sepal_width</th>
<th>petal_length</th>
<th>petal_width</th>
<th>species</th>
</tr>
</thead>
<tbody>
<tr>
<td>0</td>
<td>5.1</td>
<td>3.5</td>
<td>1.4</td>
<td>0.2</td>
<td>setosa</td>
</tr>
<tr>
<td>1</td>
<td>4.9</td>
<td>3.0</td>
<td>1.4</td>
<td>0.2</td>
<td>setosa</td>
</tr>
<tr>
<td>2</td>
<td>4.7</td>
<td>3.2</td>
<td>1.3</td>
<td>0.2</td>
<td>setosa</td>
</tr>
<tr>
<td>3</td>
<td>4.6</td>
<td>3.1</td>
<td>1.5</td>
<td>0.2</td>
<td>setosa</td>
</tr>
<tr>
<td>4</td>
<td>5.0</td>
<td>3.6</td>
<td>1.4</td>
<td>0.2</td>
<td>setosa</td>
</tr>
</tbody>
</table>
<p>In [4]:</p>
<p>dataset.groupby('species').size() #menampilkan row masing-masing spesies</p>
<p>Out[4]:</p>
<p>species</p>
<p>setosa        50</p>
<p>versicolor    50</p>
<p>virginica     50</p>
<p>dtype: int64</p>
<p>\4. Membuat data frame pada masing-masing spesies</p>
<p>In [5]:</p>
<p>setosa = dataset[dataset['species']=='setosa']</p>
<p>versicolor = dataset[dataset['species']=='versicolor']</p>
<p>virginica = dataset[dataset['species']=='virginica']</p>
<p>In [6]:</p>
<p>setosa.describe()</p>
<p>Out[6]:</p>
<table>
<thead>
<tr>
<th></th>
<th>sepal_length</th>
<th>sepal_width</th>
<th>petal_length</th>
<th>petal_width</th>
</tr>
</thead>
<tbody>
<tr>
<td>count</td>
<td>50.00000</td>
<td>50.000000</td>
<td>50.000000</td>
<td>50.00000</td>
</tr>
<tr>
<td>mean</td>
<td>5.00600</td>
<td>3.418000</td>
<td>1.464000</td>
<td>0.24400</td>
</tr>
<tr>
<td>std</td>
<td>0.35249</td>
<td>0.381024</td>
<td>0.173511</td>
<td>0.10721</td>
</tr>
<tr>
<td>min</td>
<td>4.30000</td>
<td>2.300000</td>
<td>1.000000</td>
<td>0.10000</td>
</tr>
<tr>
<td>25%</td>
<td>4.80000</td>
<td>3.125000</td>
<td>1.400000</td>
<td>0.20000</td>
</tr>
<tr>
<td>50%</td>
<td>5.00000</td>
<td>3.400000</td>
<td>1.500000</td>
<td>0.20000</td>
</tr>
<tr>
<td>75%</td>
<td>5.20000</td>
<td>3.675000</td>
<td>1.575000</td>
<td>0.30000</td>
</tr>
<tr>
<td>max</td>
<td>5.80000</td>
<td>4.400000</td>
<td>1.900000</td>
<td>0.60000</td>
</tr>
</tbody>
</table>
<p>In [7]:</p>
<p>virginica.describe()</p>
<p>Out[7]:</p>
<table>
<thead>
<tr>
<th></th>
<th>sepal_length</th>
<th>sepal_width</th>
<th>petal_length</th>
<th>petal_width</th>
</tr>
</thead>
<tbody>
<tr>
<td>count</td>
<td>50.00000</td>
<td>50.000000</td>
<td>50.000000</td>
<td>50.00000</td>
</tr>
<tr>
<td>mean</td>
<td>6.58800</td>
<td>2.974000</td>
<td>5.552000</td>
<td>2.02600</td>
</tr>
<tr>
<td>std</td>
<td>0.63588</td>
<td>0.322497</td>
<td>0.551895</td>
<td>0.27465</td>
</tr>
<tr>
<td>min</td>
<td>4.90000</td>
<td>2.200000</td>
<td>4.500000</td>
<td>1.40000</td>
</tr>
<tr>
<td>25%</td>
<td>6.22500</td>
<td>2.800000</td>
<td>5.100000</td>
<td>1.80000</td>
</tr>
<tr>
<td>50%</td>
<td>6.50000</td>
<td>3.000000</td>
<td>5.550000</td>
<td>2.00000</td>
</tr>
<tr>
<td>75%</td>
<td>6.90000</td>
<td>3.175000</td>
<td>5.875000</td>
<td>2.30000</td>
</tr>
<tr>
<td>max</td>
<td>7.90000</td>
<td>3.800000</td>
<td>6.900000</td>
<td>2.50000</td>
</tr>
</tbody>
</table>
<p>In [8]:</p>
<p>virginica.describe()</p>
<p>Out[8]:</p>
<table>
<thead>
<tr>
<th></th>
<th>sepal_length</th>
<th>sepal_width</th>
<th>petal_length</th>
<th>petal_width</th>
</tr>
</thead>
<tbody>
<tr>
<td>count</td>
<td>50.00000</td>
<td>50.000000</td>
<td>50.000000</td>
<td>50.00000</td>
</tr>
<tr>
<td>mean</td>
<td>6.58800</td>
<td>2.974000</td>
<td>5.552000</td>
<td>2.02600</td>
</tr>
<tr>
<td>std</td>
<td>0.63588</td>
<td>0.322497</td>
<td>0.551895</td>
<td>0.27465</td>
</tr>
<tr>
<td>min</td>
<td>4.90000</td>
<td>2.200000</td>
<td>4.500000</td>
<td>1.40000</td>
</tr>
<tr>
<td>25%</td>
<td>6.22500</td>
<td>2.800000</td>
<td>5.100000</td>
<td>1.80000</td>
</tr>
<tr>
<td>50%</td>
<td>6.50000</td>
<td>3.000000</td>
<td>5.550000</td>
<td>2.00000</td>
</tr>
<tr>
<td>75%</td>
<td>6.90000</td>
<td>3.175000</td>
<td>5.875000</td>
<td>2.30000</td>
</tr>
<tr>
<td>max</td>
<td>7.90000</td>
<td>3.800000</td>
<td>6.900000</td>
<td>2.50000</td>
</tr>
</tbody>
</table>
<p>In [9]:</p>
<p># deskripsi dataset secara kesuluruhan</p>
<p>dataset.describe()</p>
<p>Out[9]:</p>
<table>
<thead>
<tr>
<th></th>
<th>sepal_length</th>
<th>sepal_width</th>
<th>petal_length</th>
<th>petal_width</th>
</tr>
</thead>
<tbody>
<tr>
<td>count</td>
<td>150.000000</td>
<td>150.000000</td>
<td>150.000000</td>
<td>150.000000</td>
</tr>
<tr>
<td>mean</td>
<td>5.843333</td>
<td>3.054000</td>
<td>3.758667</td>
<td>1.198667</td>
</tr>
<tr>
<td>std</td>
<td>0.828066</td>
<td>0.433594</td>
<td>1.764420</td>
<td>0.763161</td>
</tr>
<tr>
<td>min</td>
<td>4.300000</td>
<td>2.000000</td>
<td>1.000000</td>
<td>0.100000</td>
</tr>
<tr>
<td>25%</td>
<td>5.100000</td>
<td>2.800000</td>
<td>1.600000</td>
<td>0.300000</td>
</tr>
<tr>
<td>50%</td>
<td>5.800000</td>
<td>3.000000</td>
<td>4.350000</td>
<td>1.300000</td>
</tr>
<tr>
<td>75%</td>
<td>6.400000</td>
<td>3.300000</td>
<td>5.100000</td>
<td>1.800000</td>
</tr>
<tr>
<td>max</td>
<td>7.900000</td>
<td>4.400000</td>
<td>6.900000</td>
<td>2.500000</td>
</tr>
</tbody>
</table>
<p>In [10]:</p>
<p># count menjelaskan bahwa semua 4 features mempunyai 150 row</p>
<p># secara umum, dari rata-rata kita dapat mengatakan bahwa sepal lebih besar dari petal</p>
<p>In [16]:</p>
<p>import matplotlib.pyplot as plt</p>
<p>plt.figure()</p>
<p>fig,ax=plt.subplots(1,2,figsize=(15, 5))</p>
<p>setosa.plot(x="sepal_length", y="sepal_width", kind="scatter",ax=ax[0],label='setosa',color='r')</p>
<p>versicolor.plot(x="sepal_length",y="sepal_width",kind="scatter",ax=ax[0],label='versicolor',color='b')</p>
<p>virginica.plot(x="sepal_length", y="sepal_width", kind="scatter", ax=ax[0], label='virginica', color='g')</p>
<p>setosa.plot(x="petal_length", y="petal_width", kind="scatter",ax=ax[1],label='setosa',color='r')</p>
<p>versicolor.plot(x="petal_length",y="petal_width",kind="scatter",ax=ax[1],label='versicolor',color='b')</p>
<p>virginica.plot(x="petal_length", y="petal_width", kind="scatter", ax=ax[1], label='virginica', color='g')</p>
<p>ax[0].set(title='Perbandingan Sepal', ylabel='Lebar sepal (cm)')</p>
<p>ax[1].set(title='Perbandingan Petal',  ylabel='Lebar petal(cm)')</p>
<p>ax[0].legend()</p>
<p>ax[1].legend()</p>
<p>plt.savefig('asda.png')</p>
<Figure size 432x288 with 0 Axes>



 Hasil grafik dari kode menggunakan jupyter notebook

Hasil grafik dari kedua kode (jupyter dan atom) memang sama saja, walaupun terdapat perbedaan, karena memang struktur file-nya sudah berbeda sejak awal. Jadi jangan bingun bilamana file yang dari sklearn tidak dapat menampilkan head() karena bentuknya dataframe.

Apabila ingin menampilkan PCA (Principal Component Analysis) dapat menggunakan code di bawah:

import matplotlib.pyplot as plt

from mpl_toolkits.mplot3d import Axes3D

from sklearn import datasets

from sklearn.decomposition import PCA

\# import some data to play with

iris = datasets.load_iris()

X = iris.data[:, :2]  # we only take the first two features.

y = iris.target

x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5

y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5

plt.figure(2, figsize=(8, 6))

plt.clf()

\# Plot the training points

plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.Set1,

​            edgecolor='k')

plt.xlabel('Sepal length')

plt.ylabel('Sepal width')

plt.xlim(x_min, x_max)

plt.ylim(y_min, y_max)

plt.xticks(())

plt.yticks(())

\# To getter a better understanding of interaction of the dimensions

\# plot the first three PCA dimensions

fig = plt.figure(1, figsize=(8, 6))

ax = Axes3D(fig, elev=-150, azim=110)

X_reduced = PCA(n_components=3).fit_transform(iris.data)

ax.scatter(X_reduced[:, 0], X_reduced[:, 1], X_reduced[:, 2], c=y,

​           cmap=plt.cm.Set1, edgecolor='k', s=40)

ax.set_title("First three PCA directions")

ax.set_xlabel("1st eigenvector")

ax.w_xaxis.set_ticklabels([])

ax.set_ylabel("2nd eigenvector")

ax.w_yaxis.set_ticklabels([])

ax.set_zlabel("3rd eigenvector")

ax.w_zaxis.set_ticklabels([])

plt.show()

![](assets\images\U.png)



plot 2d iris dan pca

Kita juga dapat meggunakan plot dari seaborn, yang mana dapat memperlihatkan hubungan bivariat antar masing-masing pasangan feature yang kita miliki. Dari pairplot tersebut kita dapat melihat bahwa pada masing-masing kombinasi *feature*, spesies Iris setosa cenderung terpisah dari kedua spesies lainnya. Minimal dari sini kita dapat mempunyai *sense* terhadap data kita apabila ingin memasukkan data baru.

sns.pairplot(iris.drop("Id", axis=1), hue="Species", size=3)

\# sns adalah modul seaborn, jika kita ingin memanggilnya cukup tambahkan diatasnya import seaborn as sns

\# iris adalah nama data kita di dalam code

![](D:\semester 4\data mining\tgs3\mkdocs-material-master\docs\assets\images\V.png)

 Pairplot data iris

.

\5. KNN dengan Python[¶](https://najibtombro.github.io/170441100101_FIRDHO-ARIANZA_KNN-/#5-knn-dengan-python)

Langkah pertama adalah memanggil data iris yang akan kita gunakan untuk membuat KNN. Misal masing-masing target/spesies kita berikan nilai yang unik, setosa=0, versicolor=1, virginica=2. Pada gambar di bawah ini dapat dilihat jika kita menggunakan K=1 dan data baru a=[1,2.7,3.6,4.2]. a mewakili masing-masing nilai feature. Hasilnya adalah 2, yaitu *Iris virginica*.

n [3]:

from sklearn.datasets import load_iris

iris=load_iris()   

x=iris.data    

y=iris.target

from sklearn.neighbors import KNeighborsClassifier

import numpy as np

knn=KNeighborsClassifier(n_neighbors=1) #define K=1

knn.fit(x,y)

a=np.array([[1.0,2.7,3.6,4.2]])

knn.predict(a)

Out[3]:

array([2])

Jika saya ganti nilai k=3 dengan nilai data a tidak saya rubah, maka hasil menjadi 1, yaitu *Iris versicolor*.*.

In [8]:

from sklearn.datasets import load_iris

iris=load_iris()   

x=iris.data    

y=iris.target

from sklearn.neighbors import KNeighborsClassifier

import numpy as np

knn=KNeighborsClassifier(n_neighbors=3) #define K=3

knn.fit(x,y)

a=np.array([[1.0,2.7,3.6,4.2]])

knn.predict(a)

Out[8]:

array([1])

Untuk menghindari overfitting, kita dapat membagi iris dataset tadi menjadi data train dan test. Perbandingannya 80%:20%, sehingga bakal ada x buat training dan testing, begitu juga dengan ‘y’ bakal ada y buat training dan testing. Jadi dari 150 total observasi pada data iris, terdapat 120 observasi pada data train dan 30 observasi pada data testing. Script pemisahan data train dan test dapat dilihat di bawah.

from sklearn.model_selection import train_test_split   X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20)

![](assets\images\X.png)

Overfitting of a classifier-validation

Cross Validation (CV) adalah metode statistik yang dapat digunakan untuk mengevaluasi kinerja model atau algoritma dimana data dipisahkan menjadi dua subset yaitu data proses pembelajaran dan data validasi / evaluasi. Model atau algoritma dilatih oleh subset pembelajaran dan divalidasi oleh subset validasi. Selanjutnya pemilihan jenis CV dapat didasarkan pada ukuran dataset. Biasanya CV K-fold digunakan karena dapat mengurangi waktu komputasi dengan tetap menjaga keakuratan estimasi.

![](D:\semester 4\data mining\tgs3\mkdocs-material-master\docs\assets\images\Y.png)

Skema 10 fold CV

Misal, data kita ada 150. Ibarat kita pake K=5, berarti kita bagi 150 data menjadi 5 partisi, isinya masing-masing 30 data. Jangan lupa, kita perlu menentukan mana yang training data dan mana yang test data. Karena perbandingannya 80:20, berarti 120 data adalah training data dan 30 sisanya adalah test data. Berdasarkan 5 partisi tadi, berarti bakal ada 4 partisi x 30 data = 120 training data. Dan sisanya ada 1 partisi test data berisi 30 data. Kemudian, experimen menggunakan data yang udah di partisi-partisi bakal diulang 5 kali (K=5). Tapi posisi partisi Test data berbeda ditiap iterasinya. Misal di iterasi pertama Test nya di posisi partisi awal, terus iterasi partisi kedua Test-nya di posisi kedua, dan seterusnya, pokonya gaboleh sama.

In [14]:

import numpy as np  

import matplotlib.pyplot as plt  

import pandas as pd

In [15]:

url = "https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data"



\# Assign colum names to the dataset

names = ['sepal-length', 'sepal-width', 'petal-length', 'petal-width', 'Class']



\# Read dataset to pandas dataframe

dataset = pd.read_csv(url, names=names)

In [18]:

X = dataset.iloc[:, :-1].values  

y = dataset.iloc[:, 4].values

In [19]:

from sklearn.model_selection import train_test_split  

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20)

In [20]:

from sklearn.preprocessing import StandardScaler  

scaler = StandardScaler()  

scaler.fit(X_train)



X_train = scaler.transform(X_train)  

X_test = scaler.transform(X_test)

In [21]:

from sklearn.neighbors import KNeighborsClassifier  

classifier = KNeighborsClassifier(n_neighbors=5)  

classifier.fit(X_train, y_train)

Out[21]:

KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',

​           metric_params=None, n_jobs=None, n_neighbors=5, p=2,

​           weights='uniform')

In [23]:

y_pred = classifier.predict(X_test)

In [25]:

from sklearn.metrics import classification_report, confusion_matrix  

print(confusion_matrix(y_test, y_pred))  

print(classification_report(y_test, y_pred))

[[11  1  0]

 [ 0  8  0]

 [ 0  0 10]]

​                 precision    recall  f1-score   support



​    Iris-setosa       1.00      0.92      0.96        12

Iris-versicolor       0.89      1.00      0.94         8

 Iris-virginica       1.00      1.00      1.00        10



​      micro avg       0.97      0.97      0.97        30

​      macro avg       0.96      0.97      0.97        30

   weighted avg       0.97      0.97      0.97        30

Hasil running code diatas menunjukkan bahwa algoritma KNN yang telah kita lakukan dapat mengklasifikasikan semua observasi (30) dalam set test dengan akurasi 97%, yang artinya susdah sangat bagus. Meskipun algoritma KNN dilakukan sangat baik dengan dataset ini, jangan berharap hasil yang sama dengan semua dataset. KNN tidak selalu berkinerja baik dengan data yang memiliki dimensi tinggi atau yang memiliki feature yang sangat kategoris.

\6. Menentukan Nilai K[¶](https://najibtombro.github.io/170441100101_FIRDHO-ARIANZA_KNN-/#6-menentukan-nilai-k)

Nah sekarang kita akan mencari tahu nilai K berapa yang akan menghasilkan akurasi tinggi. Karena kita tahu sendiri pada algoritma KNN, penentuan nilai K sangatlah krusial. Kita akan melihat nilai K dari 1–40. Berikut code yang dapat kita gunakan:

In [14]:

import numpy as np  

import matplotlib.pyplot as plt  

import pandas as pd

In [15]:

url = "https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data"



\# Assign colum names to the dataset

names = ['sepal-length', 'sepal-width', 'petal-length', 'petal-width', 'Class']



\# Read dataset to pandas dataframe

dataset = pd.read_csv(url, names=names)

In [18]:

X = dataset.iloc[:, :-1].values  

y = dataset.iloc[:, 4].values

In [19]:

from sklearn.model_selection import train_test_split  

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20)

In [20]:

from sklearn.preprocessing import StandardScaler  

scaler = StandardScaler()  

scaler.fit(X_train)



X_train = scaler.transform(X_train)  

X_test = scaler.transform(X_test)

In [21]:

from sklearn.neighbors import KNeighborsClassifier  

classifier = KNeighborsClassifier(n_neighbors=5)  

classifier.fit(X_train, y_train)

Out[21]:

KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',

​           metric_params=None, n_jobs=None, n_neighbors=5, p=2,

​           weights='uniform')

In [23]:

y_pred = classifier.predict(X_test)

In [26]:

from sklearn.metrics import classification_report, confusion_matrix  

print(confusion_matrix(y_test, y_pred))  

print(classification_report(y_test, y_pred))

[[11  1  0]

 [ 0  8  0]

 [ 0  0 10]]

​                 precision    recall  f1-score   support



​    Iris-setosa       1.00      0.92      0.96        12

Iris-versicolor       0.89      1.00      0.94         8

 Iris-virginica       1.00      1.00      1.00        10



​      micro avg       0.97      0.97      0.97        30

​      macro avg       0.96      0.97      0.97        30

   weighted avg       0.97      0.97      0.97        30

In [30]:

error = []



\# Calculating error for K values between 1 and 40

for i in range(1, 40):  

​    knn = KNeighborsClassifier(n_neighbors=i)

​    knn.fit(X_train, y_train)

​    pred_i = knn.predict(X_test)

​    error.append(np.mean(pred_i != y_test))

In [31]:

plt.figure(figsize=(12, 6))  

plt.plot(range(1, 40), error, color='red', linestyle='dashed', marker='o',  

​         markerfacecolor='blue', markersize=10)

plt.title('Error Rate Nilai K')  

plt.xlabel('Nilai K')  

plt.ylabel('Error rata-rata')

Out[31]:

Text(0, 0.5, 'Error rata-rata')

Gambarrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr



Hasil tunning nilai K

Dari hasil di atas kita dapat melihat bahwa yang memiliki nilai error rata-rata 0 adalah ketika nilai K sebesar 4, 6–24. Hasil ini dapat dijadikan pedoman ketika kita ingin menggunakan nilai K yang memiliki akurasi tinggi. Dari situ kita dapat meminimalisir kesalahan prediksi.

.

## 7. Menampilkan hasil klarifikasi Data Iris[¶](https://najibtombro.github.io/170441100101_FIRDHO-ARIANZA_KNN-/#7-menampilkan-hasil-klarifikasi-data-iris)

Kita dapat menampilkan plot hasil klasifikasi data iris. Kemudian kita dapat melihat perubahan yang terjadi ketika nilai K berbeda. Langkah ini sangat membantu jika kita tidak dapat membayangkan bagaimana data tersebut bekerja.Klasifikasi knn

Kita juga dapat membuat code ketika menjadi dinamis, sehingga memungkinkan adanya interaksi ketika kita memasukkan data. Hanya dengan menambahkan beberapa perintah logika kita dapat melakukan hal tersebut.

<pre class="codehilite"><code>import numpy as np
from sklearn import neighbors, datasets
from sklearn import preprocessing 

n_neighbors = 6

# import some data to play with
iris = datasets.load_iris()

# prepare data
X = iris.data[:, :2]  
y = iris.target
h = .02 

# we create an instance of Neighbours Classifier and fit the data.
clf = neighbors.KNeighborsClassifier(n_neighbors, weights='distance')
clf.fit(X, y)

# make prediction
sl = input('Enter sepal length (cm): ')
sw = input('Enter sepal width (cm): ')
dataClass = clf.predict([[sl,sw]])
print('Prediction: '),

if dataClass == 0:
    print('Iris Setosa')
elif dataClass == 1:
    print('Iris Versicolour')
else:
    print('Iris Virginica')</code></pre>

![img](file:///C:/Users/TOSHIBA/AppData/Local/Temp/msohtmlclip1/01/clip_image021.gif)

 Hasil code

Sekian yang dapat saya sampaikan kurang lebihnya saya mohon maaf

.

## 8. Daftar Pustaka[¶](https://najibtombro.github.io/170441100101_FIRDHO-ARIANZA_KNN-/#8-daftar-pustaka)

Sumber :

<https://www.advernesia.com/blog/data-science/pengertian-dan-cara-kerja-algoritma-k-nearest-neighbours-knn/>

<https://medium.com/bosbouw/k-nearest-neighbors-menggunakan-python-bd3652ba1e70>

Klasifikasi Dokumen Menggunakan Metode k-Nearest Neighbor (kNN) dengan Information Gain

Document Classification using k-Nearest Neighbor (kNN) Method with Information Gain (Jurnal Telkomuniversit )
                
                  
                
              
              
                


              
            </article>
          </div>
        </div>
      </main>
      
        
<footer class="md-footer">
  
    <div class="md-footer-nav">
      <nav class="md-footer-nav__inner md-grid">
        
          <a href=".." title="K-Means  data iris" class="md-flex md-footer-nav__link md-footer-nav__link--prev" rel="prev">
            <div class="md-flex__cell md-flex__cell--shrink">
              <i class="md-icon md-icon--arrow-back md-footer-nav__button"></i>
            </div>
            <div class="md-flex__cell md-flex__cell--stretch md-footer-nav__title">
              <span class="md-flex__ellipsis">
                <span class="md-footer-nav__direction">
                  Previous
                </span>
                K-Means  data iris
              </span>
            </div>
          </a>
        
        
          <a href="../pohon_keputusan/" title="Pohon Keputusan" class="md-flex md-footer-nav__link md-footer-nav__link--next" rel="next">
            <div class="md-flex__cell md-flex__cell--stretch md-footer-nav__title">
              <span class="md-flex__ellipsis">
                <span class="md-footer-nav__direction">
                  Next
                </span>
                Pohon Keputusan
              </span>
            </div>
            <div class="md-flex__cell md-flex__cell--shrink">
              <i class="md-icon md-icon--arrow-forward md-footer-nav__button"></i>
            </div>
          </a>
        
      </nav>
    </div>
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-footer-copyright">
        
          <div class="md-footer-copyright__highlight">
            Copyright &copy; 2016 - 2019 sitimusabbiahtulfadilah
          </div>
        
        powered by
        <a href="https://www.mkdocs.org">MkDocs</a>
        and
        <a href="https://squidfunk.github.io/mkdocs-material/">
          Material for MkDocs</a>
      </div>
      
  <div class="md-footer-social">
    <link rel="stylesheet" href="../assets/fonts/font-awesome.css">
    
      <a href="https://github.com/sitimusabbiahtulfadilah1234" class="md-footer-social__link fa fa-github-alt"></a>
    
  </div>

    </div>
  </div>
</footer>
      
    </div>
    
      <script src="../assets/javascripts/application.8c0d971c.js"></script>
      
      <script>app.initialize({version:"1.0.4",url:{base:".."}})</script>
      
    
  </body>
</html>